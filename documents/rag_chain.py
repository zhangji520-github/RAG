import sys
import os
# æ·»åŠ ä¸Šçº§ç›®å½•åˆ° Python è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from llm_utils import qwen_embeddings, openai_embedding, llm
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from milvus_db import MilvusVectorSave
from langchain.prompts import PromptTemplate
from markdown_parser import MarkdownParser


# Define the prompt template for generating AI responses
PROMPT_TEMPLATE = """
Human: You are an AI assistant, and you provide answers to questions by using fact based and statistical information when possible.
Use the following pieces of information to provide a concise answer to the question enclosed in <question> tags.
If you don't know the answer, just say that you don't know, don't try to make up an answer.
<context>
{context}
</context>

<question>
{question}
</question>

The response should be specific and use statistics or numbers when possible.

Assistant:"""

# Create a PromptTemplate instance with the defined template and input variables
prompt = PromptTemplate(
    template=PROMPT_TEMPLATE, input_variables=["context", "question"]
)

class RagChain:
    """è‡ªå®šä¹‰ragchain"""
    @staticmethod
    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)
    
    def run_chain(self, retrieval, question):
        # é¦–å…ˆè·å–æ£€ç´¢ç»“æœ
        retrieved_docs = retrieval.invoke(question)
        
        # æ‰“å°æ£€ç´¢ç»“æœ
        print("ğŸ” æ£€ç´¢ç»“æœ:")
        print("=" * 40)
        for i, doc in enumerate(retrieved_docs, 1):
            print(f"æ–‡æ¡£ç‰‡æ®µ {i}:")
            print(f"å†…å®¹: {doc.page_content}")
            print("-" * 30)
        
        # æ„å»ºRAGé“¾
        rag_chain = (
            {"context": retrieval | RagChain.format_docs, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )
        
        # ç”Ÿæˆç­”æ¡ˆ
        print("\nğŸ¤– ç”Ÿæˆç­”æ¡ˆ:")
        # res = rag_chain.invoke(question)
        # print(res)
        for chunk in rag_chain.stream(question):
            print(chunk, end="", flush=True)

if __name__ == "__main__":
    # 1. åŠ è½½æ–‡æ¡£æ•°æ®
    file_path = r"E:\Workspace\ai\RAG\datas\md\tech_report_z7tx05vt.md"
    parser = MarkdownParser()
    docs = parser.parse_markdown_to_documents(file_path)
    print(f"åŠ è½½äº† {len(docs)} ä¸ªæ–‡æ¡£å—")

    # 2. åˆ›å»º Milvus å‘é‡æ•°æ®åº“å¹¶æ·»åŠ æ–‡æ¡£ 
    mv = MilvusVectorSave()
    mv.create_connection(is_first=True)
    mv.add_documents(docs)
    print("æ–‡æ¡£å·²æ·»åŠ åˆ°å‘é‡æ•°æ®åº“")

    # 3. åˆ›å»º RAG é“¾å¹¶æµ‹è¯•
    rag = RagChain()
    
    # 4. è®¾ç½®æ£€ç´¢å™¨ - ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦æœç´¢
    retriever = mv.vector_stored_saved.as_retriever(
        search_type="similarity",
        search_kwargs={"k": 1}  # è¿”å›æœ€ç›¸ä¼¼çš„1ä¸ªæ–‡æ¡£å—
    )
    
    # 5. æµ‹è¯•é—®é¢˜
    test_questions = [
        "å¹²æ³•åˆ»èš€çš„ä¼˜åŠ¿ï¼Ÿ",
    ]
    
    print("\nå¼€å§‹æµ‹è¯• RAG ç³»ç»Ÿ:")
    print("=" * 50)
    
    for i, question in enumerate(test_questions, 1):
        print(f"\né—®é¢˜ {i}: {question}")
        print("-" * 30)
        
        try:
            rag.run_chain(retriever, question)
        except Exception as e:
            print(f"é”™è¯¯: {e}")
        
        print("-" * 50)
